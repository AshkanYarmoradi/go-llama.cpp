<div align="center">

<img width="300" src="./images/logo.png">

# ü¶ô go-llama.cpp

### *Blazing Fast LLM Inference in Go*

[![Go Reference](https://pkg.go.dev/badge/github.com/AshkanYarmoradi/go-llama.cpp.svg)](https://pkg.go.dev/github.com/AshkanYarmoradi/go-llama.cpp)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Go Report Card](https://goreportcard.com/badge/github.com/AshkanYarmoradi/go-llama.cpp)](https://goreportcard.com/report/github.com/AshkanYarmoradi/go-llama.cpp)

**High-performance [llama.cpp](https://github.com/ggerganov/llama.cpp) bindings for Go ‚Äî run LLMs locally with the power of C++ and the simplicity of Go.**

[Getting Started](#-quick-start) ‚Ä¢
[Features](#-features) ‚Ä¢
[API Reference](#-api-reference) ‚Ä¢
[GPU Acceleration](#-acceleration) ‚Ä¢
[Examples](#-examples)

</div>

---

## üåü About This Fork

> **Note**: The original `go-skynet/go-llama.cpp` repository was unmaintained for over a year. As the llama.cpp ecosystem evolved rapidly with new features, samplers, and breaking API changes, the Go bindings fell behind.
>
> **I decided to fork and actively maintain this project** to ensure the Go community has access to the latest llama.cpp capabilities. This fork is fully updated to support the modern llama.cpp API including the new sampler chain architecture and GGUF format.

### What's New in This Fork (December 2025)

- ‚úÖ **Updated to latest llama.cpp** ‚Äî Full compatibility with modern GGUF models
- ‚úÖ **New Sampler Chain API** ‚Äî Modern sampling architecture with composable samplers
- ‚úÖ **XTC Sampler** ‚Äî Cross-Token Coherence for improved generation quality
- ‚úÖ **DRY Sampler** ‚Äî "Don't Repeat Yourself" penalty to reduce repetition
- ‚úÖ **TopNSigma Sampler** ‚Äî Statistical sampling for better token selection
- ‚úÖ **Model Info API** ‚Äî Query model metadata (vocab size, layers, parameters, etc.)
- ‚úÖ **Chat Templates** ‚Äî Native support for model chat templates
- ‚úÖ **Fixed Build System** ‚Äî Proper static linking with all CPU optimizations

---

## üöÄ Features

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  üéØ Performance First                                           ‚îÇ
‚îÇ  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ                                           ‚îÇ
‚îÇ  ‚Ä¢ Zero-copy data passing to C++                                ‚îÇ
‚îÇ  ‚Ä¢ Minimal CGO overhead                                         ‚îÇ
‚îÇ  ‚Ä¢ Native CPU optimizations (AVX, AVX2, AVX-512)               ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  üîß Flexible Sampling                                           ‚îÇ
‚îÇ  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ                                           ‚îÇ
‚îÇ  ‚Ä¢ Temperature, Top-K, Top-P, Min-P                            ‚îÇ
‚îÇ  ‚Ä¢ Repetition & Presence Penalties                              ‚îÇ
‚îÇ  ‚Ä¢ XTC, DRY, TopNSigma (NEW!)                                  ‚îÇ
‚îÇ  ‚Ä¢ Mirostat v1 & v2                                            ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  ‚ö° GPU Acceleration                                            ‚îÇ
‚îÇ  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ                                           ‚îÇ
‚îÇ  ‚Ä¢ NVIDIA CUDA / cuBLAS                                        ‚îÇ
‚îÇ  ‚Ä¢ AMD ROCm / HIPBlas                                          ‚îÇ
‚îÇ  ‚Ä¢ Apple Metal (M1/M2/M3)                                      ‚îÇ
‚îÇ  ‚Ä¢ OpenCL / CLBlast                                            ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  üì¶ Model Support                                               ‚îÇ
‚îÇ  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ                                           ‚îÇ
‚îÇ  ‚Ä¢ All GGUF quantization formats                               ‚îÇ
‚îÇ  ‚Ä¢ LLaMA, Mistral, Qwen, Phi, and 100+ architectures          ‚îÇ
‚îÇ  ‚Ä¢ LoRA adapter loading                                        ‚îÇ
‚îÇ  ‚Ä¢ Embeddings generation                                       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## üìã Requirements

- Go 1.20+
- C/C++ compiler (GCC, Clang, or MSVC)
- CMake 3.14+
- (Optional) CUDA Toolkit for NVIDIA GPU support
- (Optional) ROCm for AMD GPU support

---

## üéØ Quick Start

### Installation

```bash
# Clone with submodules
git clone --recurse-submodules https://github.com/AshkanYarmoradi/go-llama.cpp
cd go-llama.cpp

# Build the bindings
make libbinding.a

# Run the example
LIBRARY_PATH=$PWD C_INCLUDE_PATH=$PWD go run ./examples -m "/path/to/model.gguf" -t 8
```

### Basic Usage

```go
package main

import (
    "fmt"
    llama "github.com/AshkanYarmoradi/go-llama.cpp"
)

func main() {
    // Load model
    model, err := llama.New("model.gguf",
        llama.SetContext(2048),
        llama.SetGPULayers(35),
    )
    if err != nil {
        panic(err)
    }
    defer model.Free()

    // Generate text
    response, err := model.Predict("Explain quantum computing in simple terms:",
        llama.SetTemperature(0.7),
        llama.SetTopP(0.9),
        llama.SetTokens(256),
    )
    if err != nil {
        panic(err)
    }
    
    fmt.Println(response)
}
```

---

## üéõÔ∏è API Reference

### New Sampler Options

```go
// XTC (Cross-Token Coherence) - Improves coherence between tokens
llama.SetXTC(probability, threshold float64)

// DRY (Don't Repeat Yourself) - Reduces repetitive patterns  
llama.SetDRY(multiplier, base float64, allowedLength, penaltyLastN int)

// TopNSigma - Statistical sampling based on standard deviations
llama.SetTopNSigma(n float64)
```

### Model Information API

```go
// Get comprehensive model metadata
info := model.GetModelInfo()
fmt.Printf("Model: %s\n", info.Description)
fmt.Printf("Vocabulary: %d tokens\n", info.NVocab)
fmt.Printf("Context Length: %d\n", info.NCtxTrain)
fmt.Printf("Embedding Dim: %d\n", info.NEmbd)
fmt.Printf("Layers: %d\n", info.NLayer)
fmt.Printf("Parameters: %d\n", info.NParams)
fmt.Printf("Size: %d bytes\n", info.Size)

// Get chat template for chat models
template := model.GetChatTemplate()
```

### All Sampling Options

| Option | Description | Default |
|--------|-------------|---------|
| `SetTemperature(t)` | Randomness (0.0 = deterministic) | 0.8 |
| `SetTopK(k)` | Limit to top K tokens | 40 |
| `SetTopP(p)` | Nucleus sampling threshold | 0.9 |
| `SetMinP(p)` | Minimum probability threshold | 0.05 |
| `SetRepeatPenalty(p)` | Penalize repeated tokens | 1.1 |
| `SetPresencePenalty(p)` | Penalize token presence | 0.0 |
| `SetFrequencyPenalty(p)` | Penalize token frequency | 0.0 |
| `SetXTC(prob, thresh)` | Cross-token coherence | disabled |
| `SetDRY(...)` | Don't Repeat Yourself | disabled |
| `SetTopNSigma(n)` | Statistical sigma sampling | disabled |
| `SetMirostat(mode)` | Mirostat sampling (1 or 2) | disabled |
| `SetMirostatTAU(tau)` | Mirostat target entropy | 5.0 |
| `SetMirostatETA(eta)` | Mirostat learning rate | 0.1 |

---

## ‚ö†Ô∏è Important Notes

### GGUF Format Only

This library works **exclusively** with the modern `gguf` file format. The legacy `ggml` format is no longer supported.

> Need `ggml` support? Use the legacy tag: [`pre-gguf`](https://github.com/AshkanYarmoradi/go-llama.cpp/releases/tag/pre-gguf)

### Converting Models

```bash
# Convert HuggingFace models to GGUF
python llama.cpp/convert_hf_to_gguf.py /path/to/model --outfile model.gguf

# Quantize for smaller size
./llama.cpp/build/bin/llama-quantize model.gguf model-q4_k_m.gguf Q4_K_M
```

---

## ‚ö° Acceleration

### CPU (Default)

The default build uses optimized CPU code with automatic SIMD detection.

```bash
make libbinding.a
LIBRARY_PATH=$PWD C_INCLUDE_PATH=$PWD go run ./examples -m "model.gguf" -t 8
```

### OpenBLAS

```bash
BUILD_TYPE=openblas make libbinding.a
CGO_LDFLAGS="-lopenblas" LIBRARY_PATH=$PWD C_INCLUDE_PATH=$PWD go run -tags openblas ./examples -m "model.gguf" -t 8
```

### üü¢ NVIDIA CUDA

```bash
BUILD_TYPE=cublas make libbinding.a
CGO_LDFLAGS="-lcublas -lcudart -L/usr/local/cuda/lib64/" \
  LIBRARY_PATH=$PWD C_INCLUDE_PATH=$PWD go run ./examples -m "model.gguf" -ngl 35
```

### üî¥ AMD ROCm

```bash
BUILD_TYPE=hipblas make libbinding.a
CC=/opt/rocm/llvm/bin/clang CXX=/opt/rocm/llvm/bin/clang++ \
  CGO_LDFLAGS="-O3 --hip-link --rtlib=compiler-rt -unwindlib=libgcc -lrocblas -lhipblas" \
  LIBRARY_PATH=$PWD C_INCLUDE_PATH=$PWD go run ./examples -m "model.gguf" -ngl 64
```

### üîµ Intel OpenCL

```bash
BUILD_TYPE=clblas CLBLAS_DIR=/path/to/clblast make libbinding.a
CGO_LDFLAGS="-lOpenCL -lclblast -L/usr/local/lib64/" \
  LIBRARY_PATH=$PWD C_INCLUDE_PATH=$PWD go run ./examples -m "model.gguf"
```

### üçé Apple Metal (M1/M2/M3)

```bash
BUILD_TYPE=metal make libbinding.a
CGO_LDFLAGS="-framework Foundation -framework Metal -framework MetalKit -framework MetalPerformanceShaders" \
  LIBRARY_PATH=$PWD C_INCLUDE_PATH=$PWD go build ./examples/main.go
cp build/bin/ggml-metal.metal .
./main -m "model.gguf" -ngl 1
```

---

## üìñ Examples

### Streaming Output

```go
model.SetTokenCallback(func(token string) bool {
    fmt.Print(token)
    return true // continue generation
})

model.Predict("Write a story about a robot:",
    llama.SetTokens(500),
    llama.SetTemperature(0.8),
)
```

### Embeddings

```go
model, _ := llama.New("model.gguf", llama.EnableEmbeddings())

embeddings, _ := model.Embeddings("The quick brown fox")
fmt.Printf("Vector dimension: %d\n", len(embeddings))
```

### With LoRA Adapter

```go
model, _ := llama.New("base-model.gguf",
    llama.SetLoraAdapter("adapter.bin"),
    llama.SetLoraBase("base-model.gguf"),
)
```

---

## ü§ù Contributing

Contributions are welcome! This fork is actively maintained and I'm happy to review PRs for:

- Bug fixes
- Performance improvements  
- New llama.cpp feature bindings
- Documentation improvements
- Test coverage

---

## üìö Resources

- **[llama.cpp](https://github.com/ggerganov/llama.cpp)** ‚Äî The C++ inference engine
- **[GGUF Format](https://github.com/ggerganov/ggml/blob/master/docs/gguf.md)** ‚Äî Model file format specification
- **[Hugging Face GGUF Models](https://huggingface.co/models?library=gguf)** ‚Äî Pre-quantized models

---

## üìÑ License

MIT License ‚Äî see [LICENSE](LICENSE) for details.

---

<div align="center">

**Built with ü¶ô by the Go + LLM community**

*If you find this useful, consider giving it a ‚≠ê*

</div>
