diff --git a/common/common.cpp b/common/common.cpp
index 9999999..9999998 100644
--- a/common/common.cpp
+++ b/common/common.cpp
@@ -1756,3 +1756,82 @@ float lr_opt::get_lr(float epoch) const {
     LOG_INF("epoch %.2g lr=%.2g\n", epoch, r);
     return r;
 }
+
+// Go binding compatibility functions
+
+gpt_params* create_gpt_params(const std::string& fname, const std::string& lora, const std::string& lora_base) {
+    gpt_params* lparams = new gpt_params;
+    fprintf(stderr, "%s: loading model %s\n", __func__, fname.c_str());
+    lparams->model.path = fname;
+    return lparams;
+}
+
+gpt_params* create_gpt_params_cuda(const std::string& fname) {
+    gpt_params* lparams = new gpt_params;
+    fprintf(stderr, "%s: loading model %s\n", __func__, fname.c_str());
+    lparams->model.path = fname;
+    return lparams;
+}
+
+void* load_binding_model(const char *fname, int n_ctx, int n_seed, bool memory_f16, bool mlock, bool embeddings, bool mmap, bool low_vram, int n_gpu_layers, int n_batch, const char *maingpu, const char *tensorsplit, bool numa, float rope_freq_base, float rope_freq_scale, bool mul_mat_q, const char *lora, const char *lora_base, bool perplexity) {
+    gpt_params * lparams;
+#ifdef GGML_USE_CUDA
+    lparams = create_gpt_params_cuda(fname);
+#else
+    lparams = create_gpt_params(fname, lora, lora_base);
+#endif
+
+    llama_binding_state * state = new llama_binding_state;
+    
+    lparams->n_ctx = n_ctx;
+    lparams->sampling.seed = n_seed;
+    lparams->embedding = embeddings;
+    lparams->use_mlock = mlock;
+    lparams->n_gpu_layers = n_gpu_layers;
+    lparams->use_mmap = mmap;
+    
+    if (rope_freq_base != 0.0f) {
+        lparams->rope_freq_base = rope_freq_base;
+    }
+    if (rope_freq_scale != 0.0f) {
+        lparams->rope_freq_scale = rope_freq_scale;
+    }
+    
+    lparams->model.path = fname;
+    if (maingpu[0] != '\0') {
+        lparams->main_gpu = std::stoi(maingpu);
+    }
+    
+    if (tensorsplit[0] != '\0') {
+        std::string arg_next = tensorsplit;
+        const std::regex regex{R"([,/]+)"};
+        std::sregex_token_iterator it{arg_next.begin(), arg_next.end(), regex, -1};
+        std::vector<std::string> split_arg{it, {}};
+        const size_t max_split = sizeof(lparams->tensor_split) / sizeof(lparams->tensor_split[0]);
+        for (size_t i = 0; i < max_split && i < split_arg.size(); ++i) {
+            lparams->tensor_split[i] = std::stof(split_arg[i]);
+        }
+    }
+    
+    lparams->n_batch = n_batch;
+    
+    if (numa) {
+        lparams->numa = GGML_NUMA_STRATEGY_DISTRIBUTE;
+    }
+    
+    llama_backend_init();
+    
+    common_init_result init_result = common_init_from_params(*lparams);
+    
+    if (!init_result.model) {
+        fprintf(stderr, "%s: error: unable to load model\n", __func__);
+        delete state;
+        delete lparams;
+        return nullptr;
+    }
+    
+    state->ctx = init_result.context.release();
+    state->model = init_result.model.release();
+    
+    delete lparams;
+    return state;
+}
diff --git a/common/common.h b/common/common.h
index 9999999..9999998 100644
--- a/common/common.h
+++ b/common/common.h
@@ -808,3 +808,15 @@ ggml_opt_dataset_t common_opt_dataset_init(struct llama_context * ctx, const std
 
 // "adamw" or "sgd" (case insensitive)
 enum ggml_opt_optimizer_type common_opt_get_optimizer(const char *);
+
+// Go binding compatibility types and functions
+typedef common_params gpt_params;
+
+struct llama_binding_state {
+    llama_context * ctx;
+    llama_model * model;
+};
+
+void* load_binding_model(const char *fname, int n_ctx, int n_seed, bool memory_f16, bool mlock, bool embeddings, bool mmap, bool low_vram, int n_gpu_layers, int n_batch, const char *maingpu, const char *tensorsplit, bool numa, float rope_freq_base, float rope_freq_scale, bool mul_mat_q, const char *lora, const char *lora_base, bool perplexity);
+
+gpt_params* create_gpt_params(const std::string& fname, const std::string& lora, const std::string& lora_base);
