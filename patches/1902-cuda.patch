diff --git a/common/common.cpp b/common/common.cpp
index abcd123..efgh456 100644
--- a/common/common.cpp
+++ b/common/common.cpp
@@ -1756,3 +1756,75 @@ float lr_opt::get_lr(float epoch) const {
     LOG_INF("epoch %.2g lr=%.2g\n", epoch, r);
     return r;
 }
+
+// go-llama.cpp binding functions
+// Updated for new llama.cpp API (common_params instead of gpt_params)
+
+static common_params* create_common_params(const std::string& fname, const std::string& lora) {
+    common_params* lparams = new common_params;
+    fprintf(stderr, "%s: loading model %s\n", __func__, fname.c_str());
+
+    lparams->model.path = fname;
+    if (!lora.empty()) {
+        common_adapter_lora_info lora_info;
+        lora_info.path = lora;
+        lora_info.scale = 1.0f;
+        lparams->lora_adapters.push_back(lora_info);
+    }
+    return lparams;
+}
+
+void* load_binding_model(const char *fname, int n_ctx, int n_seed, bool memory_f16, bool mlock, bool embeddings, bool mmap, bool low_vram, int n_gpu_layers, int n_batch, const char *maingpu, const char *tensorsplit, bool numa, float rope_freq_base, float rope_freq_scale, bool mul_mat_q, const char *lora, const char *lora_base, bool perplexity) {
+    (void)memory_f16;
+    (void)low_vram;
+    (void)mul_mat_q;
+    (void)perplexity;
+    (void)numa;
+    (void)rope_freq_base;
+    (void)rope_freq_scale;
+    (void)lora_base;
+
+    common_params* lparams = create_common_params(fname, lora ? lora : "");
+    
+    llama_binding_state* state = new llama_binding_state;
+    
+    lparams->n_ctx = n_ctx;
+    lparams->sampling.seed = n_seed;
+    lparams->embedding = embeddings;
+    lparams->use_mlock = mlock;
+    lparams->n_gpu_layers = n_gpu_layers;
+    lparams->use_mmap = mmap;
+    lparams->n_batch = n_batch;
+
+    if (maingpu && maingpu[0] != '\0') {
+        lparams->main_gpu = std::stoi(maingpu);
+    }
+
+    if (tensorsplit && tensorsplit[0] != '\0') {
+        std::string arg_next = tensorsplit;
+        const std::regex regex{R"([,/]+)"};
+        std::sregex_token_iterator it{arg_next.begin(), arg_next.end(), regex, -1};
+        std::vector<std::string> split_arg{it, {}};
+        
+        size_t max_devices = llama_max_devices();
+        for (size_t i = 0; i < split_arg.size() && i < max_devices; ++i) {
+            lparams->tensor_split[i] = std::stof(split_arg[i]);
+        }
+    }
+
+    llama_backend_init();
+
+    common_init_result init_result = common_init_from_params(*lparams);
+    if (!init_result.model || !init_result.context) {
+        fprintf(stderr, "%s: error: unable to load model\n", __func__);
+        delete lparams;
+        delete state;
+        return nullptr;
+    }
+    
+    state->model = init_result.model.release();
+    state->ctx = init_result.context.release();
+    
+    delete lparams;
+    return state;
+}
diff --git a/common/common.h b/common/common.h
index abcd123..efgh456 100644
--- a/common/common.h
+++ b/common/common.h
@@ -808,3 +808,11 @@ ggml_opt_dataset_t common_opt_dataset_init(struct llama_context * ctx, const std
 
 // "adamw" or "sgd" (case insensitive)
 enum ggml_opt_optimizer_type common_opt_get_optimizer(const char *);
+
+// go-llama.cpp binding structures and functions
+struct llama_binding_state {
+    llama_context * ctx;
+    llama_model * model;
+};
+
+void* load_binding_model(const char *fname, int n_ctx, int n_seed, bool memory_f16, bool mlock, bool embeddings, bool mmap, bool low_vram, int n_gpu_layers, int n_batch, const char *maingpu, const char *tensorsplit, bool numa, float rope_freq_base, float rope_freq_scale, bool mul_mat_q, const char *lora, const char *lora_base, bool perplexity);
